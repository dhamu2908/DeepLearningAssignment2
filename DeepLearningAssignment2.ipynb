{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiI+iMaVzGBiQNBHmYRist",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhamu2908/DeepLearningAssignment2/blob/main/DeepLearningAssignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15hMWUxxbIfJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "387e4f10-e7e7-4892-bba8-e4f67ade5a40"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nDeep Learning Assignment 2\\nCS24M027 Dhamodharan Muthu Muniyandi\\nIIT MADRAS\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "\"\"\"\n",
        "Deep Learning Assignment 2\n",
        "CS24M027 Dhamodharan Muthu Muniyandi\n",
        "IIT MADRAS\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "#Data Accessing check\n",
        "\n",
        "print(\"Exploring contents of /kaggle/input/dldata:\")\n",
        "dataset_path = \"/kaggle/input/dldata/inaturalist_12K\"\n",
        "train_path = os.path.join(dataset_path, \"train\")\n",
        "val_path = os.path.join(dataset_path, \"val\")\n",
        "\n",
        "# Verifying the existence of directories\n",
        "is_train_available = os.path.isdir(train_path)\n",
        "is_val_available = os.path.isdir(val_path)\n",
        "\n",
        "\n",
        "print(f\"Train directory found: {is_train_available}\")\n",
        "print(f\"Validation directory found: {is_val_available}\")\n",
        "\n",
        "if is_train_available:\n",
        "    print(\"Sample items in training folder:\", os.listdir(train_path)[:5])\n",
        "\n",
        "if is_val_available:\n",
        "    print(\"Sample items in validation folder:\", os.listdir(val_path)[:5])"
      ],
      "metadata": {
        "id": "k2qREJMUkC-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import wandb"
      ],
      "metadata": {
        "id": "xs61B-TSkDAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize WandB\n",
        "wandb.login(key = \"\" )"
      ],
      "metadata": {
        "id": "VqpIvaD6kDC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training configuration parameters\n",
        "config = {\n",
        "    \"num_epochs\": 10,\n",
        "    \"lr\": 1e-4,\n",
        "    \"batch_sz\": 32,\n",
        "    \"backbone\": \"resnet50\",\n",
        "    \"unfreeze_layers\": 1\n",
        "}\n",
        "\n",
        "IMG_DIM = 224\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "# Function to divide dataset into training and validation sets while preserving class balance\n",
        "def stratified_split(dataset, train_frac):\n",
        "    train_ids = []\n",
        "    val_ids = []\n",
        "\n",
        "    # Index ranges for each class (based on specific dataset arrangement)\n",
        "    class_boundaries = [\n",
        "        (0, 999), (1000, 1999), (2000, 2999), (3000, 3999), (4000, 4998),\n",
        "        (4999, 5998), (5999, 6998), (6999, 7998), (7999, 8998), (8999, 9998)\n",
        "    ]\n",
        "\n",
        "    for lower, upper in class_boundaries:\n",
        "        indices = list(range(lower, upper + 1))\n",
        "        split_point = int(len(indices) * train_frac)\n",
        "        train_ids.extend(indices[:split_point])\n",
        "        val_ids.extend(indices[split_point:])\n",
        "\n",
        "    training_data = Subset(dataset, train_ids)\n",
        "    validation_data = Subset(dataset, val_ids)\n",
        "\n",
        "    return training_data, validation_data\n"
      ],
      "metadata": {
        "id": "sANVJlZ5kDFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Responsible for creating data loaders and returning dataset statistics\n",
        "def build_data_pipeline(config):\n",
        "    size = (IMAGE_DIMENSION, IMAGE_DIMENSION)\n",
        "\n",
        "    def get_transform():\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize(size),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "    # Define paths for training and testing datasets\n",
        "    path_train = \"/kaggle/input/nature1/inaturalist_12K/train\"\n",
        "    path_test = \"/kaggle/input/nature1/inaturalist_12K/val\"\n",
        "\n",
        "    # Load raw datasets\n",
        "    complete_train_dataset = ImageFolder(root=path_train, transform=get_transform())\n",
        "    test_dataset = ImageFolder(root=path_test, transform=get_transform())\n",
        "\n",
        "    # Split training set into train and validation sets\n",
        "    splitter = DatasetSplitter(complete_train_dataset, 0.8)\n",
        "    train_subset, val_subset = splitter.perform_split()\n",
        "\n",
        "    # Create data loaders\n",
        "    bs = config[\"batch_size\"]\n",
        "    loader_train = DataLoader(train_subset, batch_size=bs, shuffle=True)\n",
        "    loader_val = DataLoader(val_subset, batch_size=bs, shuffle=True)\n",
        "    loader_test = DataLoader(test_dataset, batch_size=bs, shuffle=True)\n",
        "\n",
        "    # Return everything in a structured dictionary\n",
        "    return {\n",
        "        \"train_size\": len(train_subset),\n",
        "        \"val_size\": len(val_subset),\n",
        "        \"test_size\": len(test_dataset),\n",
        "        \"train_loader\": loader_train,\n",
        "        \"val_loader\": loader_val,\n",
        "        \"test_loader\": loader_test\n",
        "    }\n"
      ],
      "metadata": {
        "id": "EGGwEEbJkDH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Constructs and customizes a ResNet-50 model based on the provided configuration\n",
        "def build_resnet_model(config):\n",
        "    from torchvision.models import resnet50\n",
        "\n",
        "    # Load pretrained ResNet-50 architecture\n",
        "    resnet = resnet50(weights=\"IMAGENET1K_V1\")\n",
        "    input_features = resnet.fc.in_features\n",
        "\n",
        "    # Replace the final layer to match the number of output classes\n",
        "    resnet.fc = torch.nn.Linear(input_features, TOTAL_CLASSES)\n",
        "\n",
        "    # Initially freeze all layers\n",
        "    for param in resnet.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Selectively unfreeze last 'k' layers\n",
        "    unfreeze_count = config[\"unfreeze_layers\"]\n",
        "    if unfreeze_count > 0:\n",
        "        for param in list(resnet.parameters())[-unfreeze_count:]:\n",
        "            param.requires_grad = True\n",
        "\n",
        "    return resnet\n"
      ],
      "metadata": {
        "id": "AP9gXO2qkDKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trains a model using the provided configuration and dataset, logs metrics to Weights & Biases\n",
        "def run_training(config, dataset_bundle):\n",
        "    import wandb\n",
        "\n",
        "    # Set device context\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Model setup\n",
        "    model = build_resnet_model(config)\n",
        "    model = torch.nn.DataParallel(model, device_ids=[0]).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
        "\n",
        "    # Load training and validation data\n",
        "    train_loader = dataset_bundle[\"train_loader\"]\n",
        "    val_loader = dataset_bundle[\"val_loader\"]\n",
        "    n_train = dataset_bundle[\"train_size\"]\n",
        "    n_val = dataset_bundle[\"val_size\"]\n",
        "\n",
        "    for ep in range(config[\"num_epochs\"]):\n",
        "        model.train()\n",
        "        train_loss_accumulator = 0.0\n",
        "        correct_train_preds = 0\n",
        "\n",
        "        for step, (x_batch, y_batch) in enumerate(train_loader):\n",
        "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(x_batch)\n",
        "            loss = criterion(logits, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss_accumulator += loss.item()\n",
        "            correct_train_preds += (logits.argmax(dim=1) == y_batch).sum().item()\n",
        "\n",
        "            if step % 10 == 0:\n",
        "                batch_acc = (logits.argmax(1) == y_batch).float().mean().item()\n",
        "                print(f\"[Epoch {ep} | Batch {step}] Accuracy: {batch_acc:.4f}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        model.eval()\n",
        "        val_loss_accumulator = 0.0\n",
        "        correct_val_preds = 0\n",
        "        with torch.no_grad():\n",
        "            for x_val, y_val in val_loader:\n",
        "                x_val, y_val = x_val.to(device), y_val.to(device)\n",
        "                val_logits = model(x_val)\n",
        "                loss = criterion(val_logits, y_val)\n",
        "                val_loss_accumulator += loss.item()\n",
        "                correct_val_preds += (val_logits.argmax(1) == y_val).sum().item()\n",
        "\n",
        "        # Calculate and log metrics\n",
        "        train_accuracy = correct_train_preds / n_train\n",
        "        train_loss = train_loss_accumulator / len(train_loader)\n",
        "        val_accuracy = correct_val_preds / n_val\n",
        "        val_loss = val_loss_accumulator / len(val_loader)\n",
        "\n",
        "        print(f\"Epoch {ep} | Train Acc: {train_accuracy:.4f} | Train Loss: {train_loss:.4f} | Val Acc: {val_accuracy:.4f} | Val Loss: {val_loss:.4f}\")\n",
        "        wandb.log({\n",
        "            \"epoch\": ep,\n",
        "            \"train_accuracy\": train_accuracy,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"val_accuracy\": val_accuracy,\n",
        "            \"val_loss\": val_loss\n",
        "        })\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "    torch.save(model.state_dict(), \"./model.pth\")\n"
      ],
      "metadata": {
        "id": "wfR1YU6okDLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6K4Ma1LmkDNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7GHZlJfIkDP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rLu7ShPWkDTa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}