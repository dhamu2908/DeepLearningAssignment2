{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN32OQdtCdjR50XJfLps+FG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhamu2908/DeepLearningAssignment2/blob/main/DeepLearningAssignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15hMWUxxbIfJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "387e4f10-e7e7-4892-bba8-e4f67ade5a40"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nDeep Learning Assignment 2\\nCS24M027 Dhamodharan Muthu Muniyandi\\nIIT MADRAS\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "\"\"\"\n",
        "Deep Learning Assignment 2\n",
        "CS24M027 Dhamodharan Muthu Muniyandi\n",
        "IIT MADRAS\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "#Data Accessing check\n",
        "\n",
        "print(\"Exploring contents of /kaggle/input/dldata:\")\n",
        "dataset_path = \"/kaggle/input/dldata/inaturalist_12K\"\n",
        "train_path = os.path.join(dataset_path, \"train\")\n",
        "val_path = os.path.join(dataset_path, \"val\")\n",
        "\n",
        "# Verifying the existence of directories\n",
        "is_train_available = os.path.isdir(train_path)\n",
        "is_val_available = os.path.isdir(val_path)\n",
        "\n",
        "\n",
        "print(f\"Train directory found: {is_train_available}\")\n",
        "print(f\"Validation directory found: {is_val_available}\")\n",
        "\n",
        "if is_train_available:\n",
        "    print(\"Sample items in training folder:\", os.listdir(train_path)[:5])\n",
        "\n",
        "if is_val_available:\n",
        "    print(\"Sample items in validation folder:\", os.listdir(val_path)[:5])"
      ],
      "metadata": {
        "id": "k2qREJMUkC-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "import wandb"
      ],
      "metadata": {
        "id": "xs61B-TSkDAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize WandB\n",
        "wandb.login(key = \"\" )"
      ],
      "metadata": {
        "id": "VqpIvaD6kDC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training configuration parameters\n",
        "config = {\n",
        "    \"num_epochs\": 10,\n",
        "    \"lr\": 1e-4,\n",
        "    \"batch_sz\": 32,\n",
        "    \"backbone\": \"resnet50\",\n",
        "    \"unfreeze_layers\": 1\n",
        "}\n",
        "\n",
        "IMG_DIM = 224\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "# Function to divide dataset into training and validation sets while preserving class balance\n",
        "def stratified_split(dataset, train_frac):\n",
        "    train_ids = []\n",
        "    val_ids = []\n",
        "\n",
        "    # Index ranges for each class (based on specific dataset arrangement)\n",
        "    class_boundaries = [\n",
        "        (0, 999), (1000, 1999), (2000, 2999), (3000, 3999), (4000, 4998),\n",
        "        (4999, 5998), (5999, 6998), (6999, 7998), (7999, 8998), (8999, 9998)\n",
        "    ]\n",
        "\n",
        "    for lower, upper in class_boundaries:\n",
        "        indices = list(range(lower, upper + 1))\n",
        "        split_point = int(len(indices) * train_frac)\n",
        "        train_ids.extend(indices[:split_point])\n",
        "        val_ids.extend(indices[split_point:])\n",
        "\n",
        "    training_data = Subset(dataset, train_ids)\n",
        "    validation_data = Subset(dataset, val_ids)\n",
        "\n",
        "    return training_data, validation_data\n"
      ],
      "metadata": {
        "id": "sANVJlZ5kDFG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Responsible for creating data loaders and returning dataset statistics\n",
        "def build_data_pipeline(config):\n",
        "    size = (IMAGE_DIMENSION, IMAGE_DIMENSION)\n",
        "\n",
        "    def get_transform():\n",
        "        return transforms.Compose([\n",
        "            transforms.Resize(size),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "    # Define paths for training and testing datasets\n",
        "    path_train = \"/kaggle/input/nature1/inaturalist_12K/train\"\n",
        "    path_test = \"/kaggle/input/nature1/inaturalist_12K/val\"\n",
        "\n",
        "    # Load raw datasets\n",
        "    complete_train_dataset = ImageFolder(root=path_train, transform=get_transform())\n",
        "    test_dataset = ImageFolder(root=path_test, transform=get_transform())\n",
        "\n",
        "    # Split training set into train and validation sets\n",
        "    splitter = DatasetSplitter(complete_train_dataset, 0.8)\n",
        "    train_subset, val_subset = splitter.perform_split()\n",
        "\n",
        "    # Create data loaders\n",
        "    bs = config[\"batch_size\"]\n",
        "    loader_train = DataLoader(train_subset, batch_size=bs, shuffle=True)\n",
        "    loader_val = DataLoader(val_subset, batch_size=bs, shuffle=True)\n",
        "    loader_test = DataLoader(test_dataset, batch_size=bs, shuffle=True)\n",
        "\n",
        "    # Return everything in a structured dictionary\n",
        "    return {\n",
        "        \"train_size\": len(train_subset),\n",
        "        \"val_size\": len(val_subset),\n",
        "        \"test_size\": len(test_dataset),\n",
        "        \"train_loader\": loader_train,\n",
        "        \"val_loader\": loader_val,\n",
        "        \"test_loader\": loader_test\n",
        "    }\n"
      ],
      "metadata": {
        "id": "EGGwEEbJkDH7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AP9gXO2qkDKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wfR1YU6okDLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6K4Ma1LmkDNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7GHZlJfIkDP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rLu7ShPWkDTa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}